{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb1fc1d",
   "metadata": {},
   "source": [
    "# RAG System for Research Paper Q&A\n",
    "**Harshit Arora & Aditi Jha**\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) pipeline that answers questions about research papers by combining semantic search with LLM generation.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load PDF documents\n",
    "2. Split text into chunks\n",
    "3. Generate embeddings (MiniLM)\n",
    "4. Store in FAISS vector database\n",
    "5. Query with similarity search\n",
    "6. Generate answers with Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c8ff5",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52630158",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community langchain-google-genai faiss-cpu sentence-transformers pypdf google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934a643",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ea41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15186654",
   "metadata": {},
   "source": [
    "## Step 3: Configure Gemini API Key\n",
    "Set your Google Gemini API key below. You can get one for free at [Google AI Studio](https://aistudio.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Groq API key here (use environment variable or replace with your key)\n",
    "GROQ_API_KEY = \"your-groq-api-key-here\"\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "print(\"API key configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba7b8b",
   "metadata": {},
   "source": [
    "## Step 4: Load PDF Documents\n",
    "Load all 5 research papers from the `research_papers/` directory using PyPDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116728a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FOLDER = \"research_papers\"\n",
    "\n",
    "all_documents = []\n",
    "pdf_files = sorted([f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")])\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    all_documents.extend(documents)\n",
    "    print(f\"  - {pdf_file}: {len(documents)} pages\")\n",
    "\n",
    "print(f\"\\nTotal pages loaded: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc3050",
   "metadata": {},
   "source": [
    "## Step 5: Split Text into Chunks\n",
    "Use `RecursiveCharacterTextSplitter` with:\n",
    "- **Chunk size**: 500 characters (~1 paragraph for higher precision)\n",
    "- **Chunk overlap**: 100 characters (prevents information loss at boundaries)\n",
    "- **Separators**: paragraph → newline → space → character (respects natural text boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(f\"Average chunk length: {sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
    "print(f\"\\nSample chunk (first 300 chars):\")\n",
    "print(f\"---\")\n",
    "print(chunks[0].page_content[:300])\n",
    "print(f\"---\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273db61d",
   "metadata": {},
   "source": [
    "## Step 6: Generate Embeddings with MiniLM\n",
    "Use `sentence-transformers/all-MiniLM-L6-v2` to convert each chunk into a 384-dimensional vector.\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Model | all-MiniLM-L6-v2 |\n",
    "| Dimensions | 384 |\n",
    "| Max Sequence Length | 256 tokens |\n",
    "| Model Size | ~80 MB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18605490",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"test sentence\")\n",
    "print(f\"Embedding model loaded successfully!\")\n",
    "print(f\"Embedding dimensions: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91995094",
   "metadata": {},
   "source": [
    "## Step 7: Create FAISS Vector Store\n",
    "Store all chunk embeddings in a FAISS index for efficient similarity search.\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Vector Store | FAISS |\n",
    "| Index Type | Flat L2 (exact search) |\n",
    "| Persistence | Saved to disk (`faiss_index/`) |\n",
    "| Search Type | Similarity search (top-k) |\n",
    "| k Value | 6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "\n",
    "print(\"Creating FAISS vector store...\")\n",
    "start_time = time.time()\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Vector store created in {elapsed:.2f}s\")\n",
    "print(f\"Total vectors stored: {vectorstore.index.ntotal}\")\n",
    "\n",
    "# Save to disk for reuse\n",
    "vectorstore.save_local(FAISS_INDEX_PATH)\n",
    "print(f\"Index saved to '{FAISS_INDEX_PATH}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ef500",
   "metadata": {},
   "source": [
    "## Step 8: Initialize Groq LLM\n",
    "Set up the Groq LLM (Llama 3.3 70B) for answer generation. Groq provides free API access with generous rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=model_name,\n",
    "    temperature=0.3,\n",
    "    groq_api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")\n",
    "\n",
    "print(f\"Groq LLM initialized: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9106971",
   "metadata": {},
   "source": [
    "## Step 9: Create Prompt Template\n",
    "Design a prompt that instructs Gemini to answer questions based only on the retrieved context, with source citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a helpful research assistant. Use the following pieces of context from research papers to answer the question. \n",
    "If you don't know the answer based on the context, say \"I don't have enough information in the provided papers to answer this question.\"\n",
    "\n",
    "Always cite which paper(s) you're referencing in your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (with citations):\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"Prompt template created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff8a2d",
   "metadata": {},
   "source": [
    "## Step 10: Build RetrievalQA Chain\n",
    "Combine the retriever (FAISS with top-k=10) and the LLM (Groq) into a LangChain RetrievalQA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain built successfully!\")\n",
    "print(f\"  - Retriever: FAISS (top-k=10)\")\n",
    "print(f\"  - LLM: {model_name}\")\n",
    "print(f\"  - Chain type: stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc57e9",
   "metadata": {},
   "source": [
    "## Step 11: Run Test Queries\n",
    "Test the RAG pipeline with sample questions about the research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9adaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    \"\"\"Ask a question and display the answer with source citations.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nA: {result['result']}\")\n",
    "    print(f\"\\n[Response time: {elapsed:.2f}s]\")\n",
    "    \n",
    "    print(f\"\\n--- Source Documents ---\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        page = doc.metadata.get('page', 'N/A')\n",
    "        print(f\"  [{i}] {os.path.basename(source)} (Page {page})\")\n",
    "        print(f\"      {doc.page_content[:150]}...\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Transformer Architecture\n",
    "result1 = ask_question(\"What is the Transformer architecture and its key components?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ece6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: BERT\n",
    "result2 = ask_question(\"What is BERT and how does it differ from previous models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Attention Mechanism\n",
    "result3 = ask_question(\"Explain the attention mechanism in neural networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c329d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional test queries\n",
    "result4 = ask_question(\"What is Retrieval-Augmented Generation (RAG) and how does it work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0578622",
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = ask_question(\"How does Sentence-BERT generate sentence embeddings?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3eecc9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This RAG pipeline successfully:\n",
    "1. **Ingested** 5 research papers from PDF format\n",
    "2. **Chunked** text into overlapping segments for optimal retrieval\n",
    "3. **Embedded** chunks using MiniLM (384-dim vectors)\n",
    "4. **Stored** embeddings in FAISS for fast similarity search\n",
    "5. **Retrieved** top-6 relevant chunks per query\n",
    "6. **Generated** accurate, grounded answers using Gemini with source citations\n",
    "\n",
    "For an interactive web interface, run the Streamlit app:\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
